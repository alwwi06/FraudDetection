{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d2ade883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "963af1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipynb\n",
      "  Downloading ipynb-0.5.1-py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: ipynb\n",
      "Successfully installed ipynb-0.5.1\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7b651439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sagemaker activation\n",
    "session = boto3.session.Session()\n",
    "region_name = session.region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = 'klarnadataset'\n",
    "role = sagemaker.get_execution_role()\n",
    "smclient = boto3.Session().client('sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0cfe9f",
   "metadata": {},
   "source": [
    "#### The famous tree-based gradient boosting XGBoost algorithm is used.\n",
    "#### The hyperparameter optimization is done in Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "93a88b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tuner import ContinuousParameter,IntegerParameter,HyperparameterTuner\n",
    "from sagemaker.session import TrainingInput\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "# model output path \n",
    "prefix = 'xgboost_model'\n",
    "output_path = 's3://{}/{}/{}/output'.format(bucket, prefix, 'xgb')\n",
    "\n",
    "\n",
    "\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=training_image, \n",
    "                                          role=role,\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.2xlarge', \n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=output_path\n",
    "                                          )\n",
    "\n",
    "estimator.set_hyperparameters(eval_metric='auc', objective='binary:logistic', num_round=100)\n",
    "\n",
    "# hyperparameters to optimized\n",
    "hyperparameter_ranges = {'eta': ContinuousParameter(0, 1), 'min_child_weight': ContinuousParameter(1, 10),\n",
    "                        'alpha': ContinuousParameter(0, 2), 'max_depth': IntegerParameter(1, 6)}\n",
    "\n",
    "# the objective function maximizes the AUC of the validation set\n",
    "tuner = HyperparameterTuner(estimator, \n",
    "                            objective_metric_name='validation:auc', \n",
    "                            objective_type='Maximize',\n",
    "                            hyperparameter_ranges=hyperparameter_ranges, \n",
    "                            max_jobs=9, \n",
    "                            max_parallel_jobs=3)\n",
    "\n",
    "\n",
    "# input training and validation datasets\n",
    "prefix = 'modeling'\n",
    "train_input = TrainingInput(\n",
    "    \"s3://{}/{}/{}\".format(bucket, prefix, \"train.csv\"), content_type=\"csv\"\n",
    ")\n",
    "validation_input = TrainingInput(\n",
    "    \"s3://{}/{}/{}\".format(bucket, prefix, \"val.csv\"), content_type=\"csv\"\n",
    ")\n",
    "\n",
    "\n",
    "tuner.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "38d3279a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-08-11 21:15:43 Starting - Preparing the instances for training\n",
      "2021-08-11 21:15:43 Downloading - Downloading input data\n",
      "2021-08-11 21:15:43 Training - Training image download completed. Training in progress.\n",
      "2021-08-11 21:15:43 Uploading - Uploading generated training model\n",
      "2021-08-11 21:15:43 Completed - Training job completed\n",
      "---------------!"
     ]
    }
   ],
   "source": [
    "# the best model is deployed\n",
    "endpoint_name = '20210811xgb2'\n",
    "xgb_model_endpoint = tuner.deploy(initial_instance_count=1, \n",
    "                                  instance_type='ml.m4.xlarge',\n",
    "                                  endpoint_name=endpoint_name,\n",
    "                                  serializer = CSVSerializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d256cc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class RealTimePredictor has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "# load the endpoint (if the notebook is restarted)\n",
    "xgb_model_endpoint = sagemaker.predictor.RealTimePredictor(endpoint_name=endpoint_name,\n",
    "                                                          serializer= CSVSerializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0ab693d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload data from S3\n",
    "prefix= 'modeling'\n",
    "\n",
    "train_location = 's3://{}/{}/{}'.format(bucket, prefix, 'train.csv')\n",
    "train_set = pd.read_csv(train_location,delimiter=',',index_col=0)\n",
    "\n",
    "val_location = 's3://{}/{}/{}'.format(bucket, prefix, 'val.csv')\n",
    "val_set = pd.read_csv(val_location,delimiter=',',index_col=0)\n",
    "\n",
    "test_location = 's3://{}/{}/{}'.format(bucket, prefix, 'test.csv')\n",
    "test_set = pd.read_csv(test_location,delimiter=',',names=[i for i in range(0,45)]) # arbitrary column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c37bb45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the prediction file to a list of probability and of 0 & 1\n",
    "def bytes_to_predictions(df_pred,threshold):\n",
    "    df_pred_list = str(df_pred)[2:-1].split(',')\n",
    "    df_pred_list_0_1 = np.array([0 if float(i) < threshold else 1 for i in df_pred_list ])\n",
    "    return df_pred_list,df_pred_list_0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fed7490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on the train, val and test sets\n",
    "train_miniset = train_set.iloc[:15000,]  # more examples give error\n",
    "train_miniset_labels = train_set.iloc[:15000,].index.values\n",
    "train_miniset_predictions = xgb_model_endpoint.predict(np.array(train_miniset))\n",
    "train_miniset_predictions_list, train_miniset_prediction_0_1 = bytes_to_predictions(train_miniset_predictions,0.5)\n",
    "\n",
    "val_labels = val_set.index.values\n",
    "val_predictions = xgb_model_endpoint.predict(np.array(val_set))\n",
    "val_predictions_list, val_predictions_0_1 = bytes_to_predictions(val_predictions,0.5)\n",
    "\n",
    "test_predictions = xgb_model_endpoint.predict(np.array(test_set))\n",
    "test_predictions_list, test_predictions_0_1 = bytes_to_predictions(test_predictions,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000daab9",
   "metadata": {},
   "source": [
    "### performance of the model on the train and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e38d57aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08adbef",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "24ea9b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8597,  760],\n",
       "       [ 946, 4697]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(train_miniset_labels,train_miniset_prediction_0_1)#,normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "38c89c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.92      0.91      9357\n",
      "         1.0       0.86      0.83      0.85      5643\n",
      "\n",
      "    accuracy                           0.89     15000\n",
      "   macro avg       0.88      0.88      0.88     15000\n",
      "weighted avg       0.89      0.89      0.89     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train_miniset_labels,train_miniset_prediction_0_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab2ca9f",
   "metadata": {},
   "source": [
    "### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "59d09b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9810,  825],\n",
       "       [ 122,  143]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(val_labels,val_predictions_0_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fadc8e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.92      0.95     10635\n",
      "         1.0       0.15      0.54      0.23       265\n",
      "\n",
      "    accuracy                           0.91     10900\n",
      "   macro avg       0.57      0.73      0.59     10900\n",
      "weighted avg       0.97      0.91      0.94     10900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_labels,val_predictions_0_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df3141d",
   "metadata": {},
   "source": [
    "#### The model is clearly overfitting and performing poorly in the validation set.\n",
    "#### For the sake of time, no further optimization is carried out, although there is plenty of room for improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a2c0c4",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e467dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test uuid from s3\n",
    "prefix = 'data_prep'\n",
    "data_key_df = 'test_uuid.csv'\n",
    "data_location = 's3://{}/{}/{}'.format(bucket, prefix, data_key_df)\n",
    "test_uuid = pd.read_csv(data_location)#,delimiter=',')\n",
    "test_uuid_list = [i.strip(\"'\") for i in  (test_uuid.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d2d65e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6f6e6c6a-2081-4e6b-8eb3-4fd89b54b2d7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f6f6d9f3-ef2b-4329-a388-c6a687f27e70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e9c39869-1bc5-4375-b627-a2df70b445ea</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6beb88a3-9641-4381-beb6-c9a208664dd0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bb89b735-72fe-42a4-ba06-d63be0f4ca36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   uuid  predictions\n",
       "0  6f6e6c6a-2081-4e6b-8eb3-4fd89b54b2d7            0\n",
       "1  f6f6d9f3-ef2b-4329-a388-c6a687f27e70            0\n",
       "2  e9c39869-1bc5-4375-b627-a2df70b445ea            0\n",
       "3  6beb88a3-9641-4381-beb6-c9a208664dd0            0\n",
       "4  bb89b735-72fe-42a4-ba06-d63be0f4ca36            0"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset of uuid and predictions\n",
    "test_preds_with_uuid = pd.DataFrame(np.array(test_uuid_list),columns=['uuid'])\n",
    "test_preds_with_uuid['predictions'] = test_predictions_0_1\n",
    "test_preds_with_uuid.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34426591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the .csv predictions file in s3\n",
    "test_preds_with_uuid.to_csv('test_predictions.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test_predictions.csv')).upload_file('test_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528aaa04",
   "metadata": {},
   "source": [
    "## REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e3213e",
   "metadata": {},
   "source": [
    "#### A REST API deployed in API GATEWAY (AWS) uses a LAMBDA function to predict a row of data.\n",
    "#### The data preprocessing pipeline is not integrated in the API yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d794d7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Non defaulter\"\n"
     ]
    }
   ],
   "source": [
    "# API URL\n",
    "url = 'https://tkqrt2qg9k.execute-api.us-east-2.amazonaws.com/test/klarna'\n",
    "# fake data\n",
    "data = {\"data\":\"0.08752,0.07697999999999999,0.047510000000000004,0.033839999999999995,0.1809,13.49,22.3,86.91,561.0,0.08752,0.07697999999999999,0.047510000000000004,0.033839999999999995,0.1809,13.49,22.3,86.91,561.0,0.08752,0.07697999999999999,0.047510000000000004,0.033839999999999995,0.1809,0.057179999999999995,0.2338,1.3530000000000002,1.735,20.2,0.004455,0.013819999999999999,0.02095,0.01184,0.01641,0.001956,15.15,31.82,99.0,698.8,0.1162,0.1711,0.2282,0.1282,0.2871,0.06917000000000001\"}\n",
    "json_str = json.dumps(data)\n",
    "\n",
    "x = requests.post(url, json_str)\n",
    "\n",
    "print(x.text)  # Defaulter or Non defaulter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a9e4a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-2\n"
     ]
    }
   ],
   "source": [
    "# call function 'preprocessing_data' from pipeline.ipynb\n",
    "from ipynb.fs.full.pipeline import preprocessing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2db521",
   "metadata": {},
   "source": [
    "#### First, the whole dataset is preprocesed. Second, the prediction takes place through the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fe0343af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload original dataset from S3\n",
    "data_key = 'dataset.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "original_dataset = pd.read_csv(data_location,delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6e5c6f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes a few minutes to pre-process the whole original dataset\n",
    "processed_dataset = preprocessing_data(original_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "357ee6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99976, 45)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows= processed_dataset.shape[0]\n",
    "processed_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e7844a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload column names after pre-proc from S3\n",
    "columns_location = 's3://{}/{}/{}'.format(bucket, prefix, 'train_columns.csv')\n",
    "columns = pd.read_csv(columns_location,delimiter=',',index_col=0)\n",
    "columns_after_prepross = columns.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "1ec7da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_random_row(df,rows):\n",
    "    #choose a random row\n",
    "    random_row_number = np.random.choice(np.arange(rows),1 )[0]\n",
    "    #convert to string\n",
    "    data = ','.join(list(df[random_row,:].astype('str')))\n",
    "    data_dict = {'data': data}\n",
    "    json_str = json.dumps(data_dict)\n",
    "    x = requests.post(url, json_str)\n",
    "    print(x.text) \n",
    "    return pd.DataFrame([df[random_row,:]],columns=columns_after_prepross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "1b6698dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Non defaulter\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_amount_added_12_24m</th>\n",
       "      <th>account_days_in_dc_12_24m</th>\n",
       "      <th>account_days_in_rem_12_24m</th>\n",
       "      <th>account_days_in_term_12_24m</th>\n",
       "      <th>account_status</th>\n",
       "      <th>age</th>\n",
       "      <th>avg_payment_span_0_12m</th>\n",
       "      <th>avg_payment_span_0_3m</th>\n",
       "      <th>has_paid</th>\n",
       "      <th>max_paid_inv_0_12m</th>\n",
       "      <th>...</th>\n",
       "      <th>merchant_group_Clothing &amp; Shoes</th>\n",
       "      <th>merchant_group_Electronics</th>\n",
       "      <th>merchant_group_Entertainment</th>\n",
       "      <th>merchant_group_Erotic Materials</th>\n",
       "      <th>merchant_group_Food &amp; Beverage</th>\n",
       "      <th>merchant_group_Health &amp; Beauty</th>\n",
       "      <th>merchant_group_Home &amp; Garden</th>\n",
       "      <th>merchant_group_Intangible products</th>\n",
       "      <th>merchant_group_Jewelry &amp; Accessories</th>\n",
       "      <th>merchant_group_Leisure, Sport &amp; Hobby</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.05</td>\n",
       "      <td>-0.934908</td>\n",
       "      <td>-0.860465</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.645522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   account_amount_added_12_24m  account_days_in_dc_12_24m  \\\n",
       "0                          0.0                        0.0   \n",
       "\n",
       "   account_days_in_rem_12_24m  account_days_in_term_12_24m  account_status  \\\n",
       "0                         0.0                          0.0             0.0   \n",
       "\n",
       "    age  avg_payment_span_0_12m  avg_payment_span_0_3m  has_paid  \\\n",
       "0  1.05               -0.934908              -0.860465       1.0   \n",
       "\n",
       "   max_paid_inv_0_12m  ...  merchant_group_Clothing & Shoes  \\\n",
       "0           -0.645522  ...                              0.0   \n",
       "\n",
       "   merchant_group_Electronics  merchant_group_Entertainment  \\\n",
       "0                         0.0                           1.0   \n",
       "\n",
       "   merchant_group_Erotic Materials  merchant_group_Food & Beverage  \\\n",
       "0                              0.0                             0.0   \n",
       "\n",
       "   merchant_group_Health & Beauty  merchant_group_Home & Garden  \\\n",
       "0                             0.0                           0.0   \n",
       "\n",
       "   merchant_group_Intangible products  merchant_group_Jewelry & Accessories  \\\n",
       "0                                 0.0                                   0.0   \n",
       "\n",
       "   merchant_group_Leisure, Sport & Hobby  \n",
       "0                                    0.0  \n",
       "\n",
       "[1 rows x 45 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_random_row = predict_random_row(processed_dataset,rows)\n",
    "df_random_row"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
