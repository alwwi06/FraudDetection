{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5966a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00bb78dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-2\n"
     ]
    }
   ],
   "source": [
    "session = boto3.session.Session()\n",
    "region_name = session.region_name\n",
    "print(region_name)\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = 'klarnadataset'\n",
    "smclient = boto3.Session().client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcd32486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original data from S3 bucket\n",
    "data_key = 'dataset.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "df = pd.read_csv(data_location,delimiter=';')\n",
    "columns_original = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4530828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(df):\n",
    "    \n",
    "    column_predictors_account_status = ['account_amount_added_12_24m', 'account_days_in_dc_12_24m',\n",
    "       'account_days_in_rem_12_24m', 'account_days_in_term_12_24m',                     \n",
    "       'age', 'avg_payment_span_0_12m','avg_payment_span_0_3m', \n",
    "       'has_paid', 'max_paid_inv_0_12m', \n",
    "       'max_paid_inv_0_24m', 'num_active_inv',\n",
    "       'num_arch_dc_0_12m', 'num_arch_dc_12_24m', \n",
    "       'num_arch_ok_0_12m', 'num_arch_ok_12_24m', \n",
    "       'num_arch_rem_0_12m', 'num_unpaid_bills',\n",
    "       'status_last_archived_0_24m', 'status_2nd_last_archived_0_24m',\n",
    "       'status_3rd_last_archived_0_24m', 'status_max_archived_0_6_months',\n",
    "       'status_max_archived_0_12_months', 'status_max_archived_0_24_months',\n",
    "       'recovery_debt', 'sum_capital_paid_account_0_12m',\n",
    "       'sum_capital_paid_account_12_24m', 'sum_paid_inv_0_12m', \n",
    "        'time_hours', 'worst_status_active_inv','num_active_div_by_paid_inv_0_12m']\n",
    "\n",
    "    column_predictors = ['account_amount_added_12_24m', 'account_days_in_dc_12_24m',\n",
    "       'account_days_in_rem_12_24m', 'account_days_in_term_12_24m',\n",
    "       'age', 'avg_payment_span_0_12m','avg_payment_span_0_3m', \n",
    "       'has_paid', 'max_paid_inv_0_12m', \n",
    "       'max_paid_inv_0_24m', 'num_active_inv',\n",
    "       'num_arch_dc_0_12m', 'num_arch_dc_12_24m', \n",
    "       'num_arch_ok_0_12m', 'num_arch_ok_12_24m', \n",
    "       'num_arch_rem_0_12m', 'num_unpaid_bills',\n",
    "       'status_last_archived_0_24m', 'status_2nd_last_archived_0_24m',\n",
    "       'status_3rd_last_archived_0_24m', 'status_max_archived_0_6_months',\n",
    "       'status_max_archived_0_12_months', 'status_max_archived_0_24_months',\n",
    "       'recovery_debt', 'sum_capital_paid_account_0_12m',\n",
    "       'sum_capital_paid_account_12_24m', 'sum_paid_inv_0_12m', \n",
    "        'time_hours', 'worst_status_active_inv']\n",
    "\n",
    "    column_predictors_worst_status = ['account_amount_added_12_24m', 'account_days_in_dc_12_24m',\n",
    "       'account_days_in_rem_12_24m', 'account_days_in_term_12_24m',                     \n",
    "       'age', 'avg_payment_span_0_12m','avg_payment_span_0_3m', \n",
    "       'has_paid', 'max_paid_inv_0_12m', 'account_status', \n",
    "       'max_paid_inv_0_24m', 'num_active_inv',\n",
    "       'num_arch_dc_0_12m', 'num_arch_dc_12_24m', \n",
    "       'num_arch_ok_0_12m', 'num_arch_ok_12_24m', \n",
    "       'num_arch_rem_0_12m', 'num_unpaid_bills',\n",
    "       'status_last_archived_0_24m', 'status_2nd_last_archived_0_24m',\n",
    "       'status_3rd_last_archived_0_24m', 'status_max_archived_0_6_months',\n",
    "       'status_max_archived_0_12_months', 'status_max_archived_0_24_months',\n",
    "       'recovery_debt', 'sum_capital_paid_account_0_12m',\n",
    "       'sum_capital_paid_account_12_24m', 'sum_paid_inv_0_12m', \n",
    "        'time_hours', 'worst_status_active_inv','num_active_div_by_paid_inv_0_12m']\n",
    "\n",
    "    medians = [0.0, 23.0, 73.0, 62.0]\n",
    "    slope = 1.2\n",
    "    \n",
    "    def inputer_account_days_in_rem_12_24m(x,medians): # imputing using its correlation with account_worst_status_12_24m\n",
    "        a = 0\n",
    "        if pd.isnull(x['account_days_in_rem_12_24m']):\n",
    "            if x['account_worst_status_12_24m'] == 1:\n",
    "                a = medians[0]\n",
    "            elif x['account_worst_status_12_24m'] == 2:\n",
    "                a = medians[1]\n",
    "            elif x['account_worst_status_12_24m'] in [3,4]:\n",
    "                a = (medians[2] + medians[3]) / 2\n",
    "        else:\n",
    "            a = x['account_days_in_rem_12_24m']\n",
    "        return a\n",
    "\n",
    "    def inputer_avg_payment_span_0_12m(x,lr_avg_payment):   # imputing values using the linear model fitted on the training set\n",
    "        a = 0\n",
    "        if pd.isnull(x['avg_payment_span_0_12m']):\n",
    "            a = lr_avg_payment.predict(np.array(x[['status_last_archived_0_24m',\n",
    "                            'status_max_archived_0_12_months',\n",
    "                            'status_max_archived_0_24_months',\n",
    "                            'num_arch_rem_0_12m']]).reshape(1,4))[0]\n",
    "            a = np.clip(a,0,None)\n",
    "        else:\n",
    "            a = x['avg_payment_span_0_12m']\n",
    "        return a\n",
    "\n",
    "    def inputer_worst_status_active_inv(x,scaler,lr_worst_status):\n",
    "        a = 0\n",
    "        if pd.isnull(x['worst_status_active_inv']):\n",
    "            X_data = scaler.transform(np.array(x[['recovery_debt', \n",
    "                                     'avg_payment_span_0_12m',\n",
    "                                     'avg_payment_span_0_3m',\n",
    "                                     'num_arch_rem_0_12m']]).reshape(1,4))\n",
    "            a = lr_worst_status.predict(X_data)[0]\n",
    "            a = np.clip(a,1.,3.)\n",
    "        else:\n",
    "            a = x['worst_status_active_inv']\n",
    "        return a\n",
    "\n",
    "    def inputer_num_active_div_by_paid_inv_0_12m(x,scaler,knn_div_by_paid):\n",
    "        a = 0\n",
    "        if pd.isnull(x['num_active_div_by_paid_inv_0_12m']):   \n",
    "            X_data_normalized = scaler.transform(np.array(x[column_predictors]).reshape(1,len(column_predictors)))\n",
    "            a = knn_div_by_paid.predict(X_data_normalized)[0]\n",
    "            a = np.clip(a,0.,None)\n",
    "        else:\n",
    "            a = x['num_active_div_by_paid_inv_0_12m']\n",
    "        return a\n",
    "\n",
    "\n",
    "    def inputer_account_status(x, feature,scaler_account,knn_account):\n",
    "        a = 0\n",
    "        if pd.isnull(x[feature]):   \n",
    "            X_data_normalized = scaler_account.transform(np.array(x[column_predictors_account_status]).reshape(1,len(column_predictors_account_status)))\n",
    "            a = knn_account.predict(X_data_normalized)[0]\n",
    "        else:\n",
    "            a = x[feature]\n",
    "        assert a != 0, 'problem with inputer_account_status'\n",
    "        return a\n",
    "\n",
    "    def email_address_format(x):\n",
    "        if x in ['no_match','Nick']:\n",
    "            x = 0 # doesn't reveal personal information\n",
    "        elif x in ['F+L','L1+F','F','F1+L','L','Initials']:\n",
    "            x = 1 # provides personal information\n",
    "        else:\n",
    "            pass\n",
    "        return x\n",
    "    \n",
    "    #concat_data = pd.read_csv(df)\n",
    "    concat_data = df\n",
    "    #drop columns\n",
    "    \n",
    "    concat_data.drop(columns=['num_arch_written_off_0_12m',\n",
    "                 'num_arch_written_off_12_24m',\n",
    "                 'account_incoming_debt_vs_paid_0_24m',\n",
    "                 'merchant_category'],inplace=True)\n",
    "    \n",
    "\n",
    "    # account_days_in_dc_12_24m & account_days_in_term_12_24m\n",
    "    concat_data['account_days_in_dc_12_24m'].fillna(value=0,inplace=True)\n",
    "    concat_data['account_days_in_term_12_24m'].fillna(value=0,inplace=True)\n",
    "    \n",
    "    # account_days_in_rem_12_24m\n",
    "    account_worst_status_medians = [0.0, 23.0, 73.0, 62.0]\n",
    "    concat_data['account_days_in_rem_12_24m'] = concat_data.apply(lambda x: inputer_account_days_in_rem_12_24m(x,medians),axis=1)\n",
    "    \n",
    "    # avg_payment_span_0_12m\n",
    "    s3 = boto3.resource('s3')\n",
    "    lr_avg_payment = pickle.loads(s3.Bucket(bucket).Object(\"lr_avg_payment.pkl\").get()['Body'].read())\n",
    "    concat_data['avg_payment_span_0_12m'] = concat_data.apply(lambda x: inputer_avg_payment_span_0_12m(x,lr_avg_payment),axis=1)\n",
    "    \n",
    "    # avg_payment_span_0_3m\n",
    "    concat_data['avg_payment_span_0_3m'] = concat_data.apply(lambda x: (slope * x['avg_payment_span_0_12m']) if pd.isnull(x['avg_payment_span_0_3m']) else x['avg_payment_span_0_3m'],axis=1)\n",
    "    \n",
    "    # worst_status_active_inv\n",
    "    # load models\n",
    "    scaler_worst_status_active_inv = pickle.loads(s3.Bucket(bucket).Object(\"scaler_worst_status_active_inv.pkl\").get()['Body'].read())\n",
    "    lr_worst_status_active_inv = pickle.loads(s3.Bucket(bucket).Object(\"lr_worst_status_active_inv.pkl\").get()['Body'].read())\n",
    "\n",
    "    concat_data['worst_status_active_inv'] = concat_data.apply(lambda x: inputer_worst_status_active_inv(x,\n",
    "                                                                                                 scaler_worst_status_active_inv,\n",
    "                                                                                                 lr_worst_status_active_inv),\n",
    "                                                               axis=1)\n",
    "    # num_active_div_by_paid_inv_0_12m\n",
    "    num_active_dim_scaler = pickle.loads(s3.Bucket(bucket).Object(\"num_active_dim_scaler.pkl\").get()['Body'].read())\n",
    "    knn_div_by_paid = pickle.loads(s3.Bucket(bucket).Object(\"knn_div_by_paid.pkl\").get()['Body'].read())\n",
    "\n",
    "\n",
    "\n",
    "    concat_data['num_active_div_by_paid_inv_0_12m'] = concat_data.apply(lambda x: inputer_num_active_div_by_paid_inv_0_12m(x,\n",
    "                                                                                                                       num_active_dim_scaler,\n",
    "                                                                                                                      knn_div_by_paid),axis=1)\n",
    "    # account_status\n",
    "    feature_name= 'account_status'\n",
    "    account_status_scaler = pickle.loads(s3.Bucket(bucket).Object('account_scaler.pkl').get()['Body'].read())\n",
    "    account_status_knn = pickle.loads(s3.Bucket(bucket).Object(\"account_status_knn.pkl\").get()['Body'].read())\n",
    "    concat_data[feature_name] = concat_data.apply(lambda x: inputer_account_status(x, feature_name,account_status_scaler,account_status_knn),axis=1)\n",
    "\n",
    "\n",
    "    # worst_status: new feature\n",
    "    concat_data['worst_status'] = pd.DataFrame(concat_data[['account_status','account_worst_status_0_3m',\n",
    "                                'account_worst_status_12_24m',\n",
    "                                'account_worst_status_3_6m',\n",
    "                                'account_worst_status_6_12m']].max(axis=1))\n",
    "    # drop some features\n",
    "    concat_data.drop(columns=['account_worst_status_0_3m',\n",
    "                                'account_worst_status_12_24m',\n",
    "                                'account_worst_status_3_6m',\n",
    "                                'account_worst_status_6_12m'],inplace=True)\n",
    "    \n",
    "    feature_name_ = 'worst_status'\n",
    "    worst_status_scaler = pickle.loads(s3.Bucket(bucket).Object('worst_status_scaler.pkl').get()['Body'].read())\n",
    "    worst_status_knn = pickle.loads(s3.Bucket(bucket).Object(\"worst_status_knn.pkl\").get()['Body'].read())\n",
    "    concat_data[feature_name] = concat_data.apply(lambda x: inputer_account_status(x, feature_name_,worst_status_scaler,worst_status_knn),axis=1)\n",
    "\n",
    "\n",
    "    # drop uuid\n",
    "    concat_data.drop(columns=['uuid'],inplace=True)\n",
    "    \n",
    "    # dummy variables for merchant_grup\n",
    "    concat_data = pd.get_dummies(concat_data, columns=[\"merchant_group\"], prefix=[\"merchant_group\"] )\n",
    "    \n",
    "    # name_in_email to categorical variables\n",
    "    concat_data['name_in_email'] = concat_data['name_in_email'].apply(lambda x: email_address_format(x))\n",
    "    \n",
    "    # 'has_paid' from boolean to categorical\n",
    "    concat_data['has_paid'] = concat_data['has_paid'].apply(lambda x: 0. if x == True else 1.)\n",
    "    \n",
    "    # normalize the whole dataset\n",
    "    robust_scaler = pickle.loads(s3.Bucket(bucket).Object('robust_scaler.pkl').get()['Body'].read())\n",
    "    concat_data = robust_scaler.transform(concat_data)\n",
    "    concat_data = concat_data[:,1:]\n",
    "    return concat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3dbbab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = df[df['default'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f3458955",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_ = test_set.copy()\n",
    "clean_test = preprocessing_data(test_set_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
